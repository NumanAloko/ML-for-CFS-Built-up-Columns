{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEdJg+uo8v9DNsctvNZ4nG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NumanAloko/ML-for-CFS-Built-up-Columns/blob/main/MLP_ML_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and testing of MLP ML model**"
      ],
      "metadata": {
        "id": "dxX9KRcSeKYP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2pIm1Cy8w-a"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################### Create Dataframe ###############################\n",
        "\n",
        "# Load the dataset\n",
        "csv_url= \"https://raw.githubusercontent.com/NumanAloko/ML-for-CFS-Built-up-Columns/refs/heads/main/CFS_Built-up_Columns_ML_Dataset.csv\"\n",
        "df = pd.read_csv(csv_url, header=0)\n",
        "df = df.dropna(how='all').dropna(axis=1, how='all')\n",
        "\n",
        "# Apply the log transformation to the features and target variable\n",
        "data_x = np.log(df[['L','t','h','b','KL_r','Py','A','Pne','P(crl_s,crd_s)']])\n",
        "df['Pt'] = np.log(df['Pt'])\n",
        "\n",
        "# Create the feature and target DataFrame\n",
        "y = pd.DataFrame(df,columns=['Pt'])\n",
        "X = pd.DataFrame(data_x, columns=['L','t','h','b','KL_r','Py','A','Pne','P(crl_s,crd_s)'])\n",
        "\n",
        "# Data Scaling\n",
        "scaler_x = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_scaled = scaler_x.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "# Split the data into training and test sets by the ratio of 70:30.\n",
        "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, X_train_index, X_test_index = train_test_split(X_scaled, y_scaled, X.index, test_size=0.3, random_state=123)"
      ],
      "metadata": {
        "id": "VkvDocry9I74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Multilayer Perceptron (MLP) 0f Artificial Neural Networks model\n",
        "mlp = MLPRegressor(max_iter=2000,tol =0.0001,random_state=123)\n",
        "\n",
        "# Hyperparameter tuning based on Grid search method\n",
        "# Define the parameter grid\n",
        "#param_grid = {\n",
        "   #'hidden_layer_sizes': [(50,), (50, 50), (100, 50), (150, 100, 50), (50, 100, 50)],\n",
        "   #'activation': ['relu', 'tanh'],\n",
        "   # 'solver': ['adam', 'sgd'],\n",
        "   # 'alpha': [0.00005, 0.0001, 0.001, 0.01],\n",
        "   # 'learning_rate': ['constant', 'adaptive'],\n",
        "   # 'early_stopping': [True],\n",
        "   # 'validation_fraction': [0.1, 0.2],  # Only relevant if early_stopping is True\n",
        "   # 'n_iter_no_change': [10, 20]  # Only relevant if early_stopping is True\n",
        "#}\n",
        "\n",
        "# Optimised Hyperparameters (Best parameters selected after tuning)\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(100,50)],\n",
        "    'activation': ['relu'],\n",
        "    'solver': ['adam'],\n",
        "    'alpha': [ 0.01],\n",
        "    'learning_rate': ['constant'],\n",
        "    'early_stopping': [True],\n",
        "    'validation_fraction': [0.1, 0.2],  # Only relevant if early_stopping is True\n",
        "    'n_iter_no_change': [10, 20]  # Only relevant if early_stopping is True\n",
        "}\n",
        "\n",
        "#Initialise GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "\n",
        "# Fit the grid search\n",
        "grid_search.fit(X_train_scaled, y_train_scaled.ravel())\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Print the best parameters\n",
        "print(best_params)"
      ],
      "metadata": {
        "id": "PKcTFvJa9I9A",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the training and test sets\n",
        "y_pred_mlp_train0 = grid_search.predict(X_train_scaled)\n",
        "y_pred_mlp_test0 = grid_search.predict(X_test_scaled)\n",
        "\n",
        "# Reverse the scaling and the log transformation on the predictions\n",
        "y_pred_mlp_train = pd.DataFrame(np.exp(scaler_y.inverse_transform(y_pred_mlp_train0.reshape(-1, 1))), index=X_train_index, columns=['Predicted'])\n",
        "y_pred_mlp_test = pd.DataFrame(np.exp(scaler_y.inverse_transform(y_pred_mlp_test0.reshape(-1, 1))), index=X_test_index, columns=['Predicted'])\n",
        "\n",
        "# Reverse the scaling and the log transformation on the target values\n",
        "y_train_de = pd.DataFrame(np.exp(scaler_y.inverse_transform(y_train_scaled)), index=X_train_index, columns=['Target'])\n",
        "y_test_de = pd.DataFrame(np.exp(scaler_y.inverse_transform(y_test_scaled)), index=X_test_index, columns=['Target'])\n",
        "\n",
        "# Concatenate the training and test predictions/targets and sort by index\n",
        "y_pred_all = pd.concat([y_pred_mlp_train, y_pred_mlp_test]).sort_index()      # Predictions of MLP Model\n",
        "y_all = pd.concat([y_train_de, y_test_de]).sort_index()                       # True Target Values"
      ],
      "metadata": {
        "id": "X08ei-xL9Ujy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################## Performance Indicators #####################################\n",
        "\n",
        "# Coefficient of determination (r2)\n",
        "r2_train = r2_score(y_train_de, y_pred_mlp_train)   # performance of the training set\n",
        "r2_test = r2_score(y_test_de, y_pred_mlp_test )     # performance of the test set\n",
        "r2_all = r2_score(y_all, y_pred_all)                # performance of all dataset\n",
        "\n",
        "# Calculation of RMSE (Root mean squared error) for training data\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train_de,  y_pred_mlp_train)) # calculate RMSE by taking the square root of MSE\n",
        "# Calculation of RMSE  (Root mean squared error) for test data\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_de,  y_pred_mlp_test)) # calculate RMSE by taking the square root of MSE\n",
        "# Calculation of RMSE  (Root mean squared error)for all data\n",
        "rmse_all = np.sqrt(mean_squared_error(y_all, y_pred_all)) # calculate RMSE by taking the square root of MSE\n",
        "print('r2 score for training = {:8.4f}'.format(r2_train))\n",
        "print('r2 score for testing = {:8.4f}'.format(r2_test))\n",
        "print('r2 score for all = {:8.4f}'.format(r2_all))\n",
        "print('RMSE score for training = {:8.4f}'.format(rmse_train))\n",
        "print('RMSE score for testing = {:8.4f}'.format(rmse_test))\n",
        "print('RMSE score for all = {:8.4f}'.format(rmse_all))\n",
        "\n",
        "# Calculation of ratios of true target values to MLP ML model's predicted values\n",
        "ratio_train_de = y_train_de.values.ravel() / y_pred_mlp_train.values.ravel()\n",
        "ratio_test_de = y_test_de.values.ravel() / y_pred_mlp_test.values.ravel()\n",
        "ratio_all = y_all.values.ravel() / y_pred_all['Predicted'].values.ravel()\n",
        "\n",
        "# Mean of ratios\n",
        "mean_ratio_train_de = ratio_train_de.mean(axis=0)\n",
        "mean_ratio_test_de = ratio_test_de.mean(axis=0)\n",
        "mean_ratio_all = np.mean(np.abs(ratio_all))\n",
        "\n",
        "# Standard deviation of ratios\n",
        "std_ratio_train_de = ratio_train_de.std(axis=0)\n",
        "std_ratio_test_de = ratio_test_de.std(axis=0)\n",
        "std_ratio_all = np.std(ratio_all)\n",
        "\n",
        "# Print mean of ratio\n",
        "print('Mean of ratio for training (de-normalized) = {:8.4f}'.format(mean_ratio_train_de.mean()))\n",
        "print('Mean of ratio for testing (de-normalized) = {:8.4f}'.format(mean_ratio_test_de.mean()))\n",
        "print('mean_ratio_all = {:8.4f}'.format(mean_ratio_all))\n",
        "\n",
        "# Print standard deviation of ratios\n",
        "print('Standard deviation of ratio for training (de-normalized) = {:8.4f}'.format(std_ratio_train_de.mean()))\n",
        "print('Standard deviation of ratio for testing (de-normalized) = {:8.4f}'.format(std_ratio_test_de.mean()))\n",
        "print('std_ratio_all = {:8.4f}'.format(std_ratio_all))"
      ],
      "metadata": {
        "id": "LMtZF2Ul9vou",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make new prediction after training of MLP ML Model**"
      ],
      "metadata": {
        "id": "YVLvFA_UePC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "# Make new prediction\n",
        "\n",
        "# Manually input the new data\n",
        "new_data = {\n",
        "    'L': [1500],               # Length(mm)\n",
        "    't': [1.5],                # Thickness of the section (mm)\n",
        "    'h': [179],                # Height of the section (mm)\n",
        "    'b': [67],                 # Flange of the section (mm)\n",
        "    'KL_r': [29.39],           # Non-dimensioanl member slenderness\n",
        "    'Py': [418.5],             # Yielding Strength of Section (kN)\n",
        "    'A': [930],                # Area (mm2) of the section\n",
        "    'Pne': [393.62],           # Global Buckling Strength of the Section (kN)\n",
        "    'P(crl_s,crd_s)': [47.74]  # Sectional (Local or Distortional) Buckling Strength of the section (kN)\n",
        "}\n",
        "\n",
        "# Create a DataFrame for the new data\n",
        "new_df = pd.DataFrame(new_data)\n",
        "\n",
        "# Apply the log transformation to the new data features\n",
        "new_data_x = np.log(new_df[['L', 't', 'h', 'b', 'KL_r', 'Py', 'A', 'Pne', 'P(crl_s,crd_s)']])\n",
        "\n",
        "# Scale the new data using the same scaler used for training data\n",
        "new_data_x_scaled = scaler_x.transform(new_data_x)\n",
        "\n",
        "# Make predictions on the new data\n",
        "new_predictions_scaled = grid_search.predict(new_data_x_scaled)\n",
        "\n",
        "# Reverse the scaling and log transformation on the predictions\n",
        "new_predictions = np.exp(scaler_y.inverse_transform(new_predictions_scaled.reshape(-1, 1)))\n",
        "\n",
        "# Create a DataFrame for the predictions\n",
        "new_predictions_df = pd.DataFrame(new_predictions, columns=['Predicted'])\n",
        "\n",
        "# Print the predictions\n",
        "print(\"Predicted Axial Strength:\", new_predictions_df['Predicted'].values)"
      ],
      "metadata": {
        "id": "9J0PrgKW92FV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretion of MLP ML model's prediction for new instance**"
      ],
      "metadata": {
        "id": "d4pk4FX0eTMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "# Interpretation of New Instance\n",
        "import shap\n",
        "\n",
        "# Initialize the explainer and compute SHAP values for the new input\n",
        "explainer = shap.KernelExplainer(grid_search.predict, shap.sample(X_train_scaled, 725))\n",
        "new_shap_values = explainer.shap_values(new_data_x_scaled)\n",
        "\n",
        "# Convert the new input data to a DataFrame with the new feature names\n",
        "new_feature_names = ['L', 't', 'h', 'b', 'KL/r', '$P_y$', 'A', '$P_{ne}$', '$P_{cr}$']\n",
        "new_input_df = pd.DataFrame(new_data_x_scaled, columns=new_feature_names)\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# The decision plot for the new instance\n",
        "shap.decision_plot(explainer.expected_value, new_shap_values, new_input_df, feature_names=new_feature_names, highlight=0, show=False)\n",
        "\n",
        "# Get the current figure and axes\n",
        "fig = plt.gcf()\n",
        "ax = plt.gca()\n",
        "\n",
        "# Set the x limits and the y label\n",
        "ax.set_xlim(-1.0, 1.0)\n",
        "ax.set_ylabel('Input Features', fontsize=14)\n",
        "ax.set_title('Interpretation of new instance', fontsize=14)\n",
        "ax.tick_params(axis='y', direction='out', length=6, width=2, colors='black')\n",
        "\n",
        "ax.grid(False)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6g62rZ5V-Co_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}