{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+jutVOZjwGlxQ4DluoA7T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NumanAloko/ML-for-CFS-Built-up-Columns/blob/main/MLP_Classification_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and testing of Multilayer Perceptron (MLP) 0f Artificial Neural Networks classification model**"
      ],
      "metadata": {
        "id": "oJOP-UC38gqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV,learning_curve\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "LXUVdMEi8wid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### Create Dataframe ###############################\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r'C:\\Users\\rq22807\\OneDrive - University of Bristol\\Desktop\\ML Paper for Revision\\GitHub\\CFS_Built-up_Columns_ML_Dataset.xlsx'\n",
        "df = pd.read_excel(file_path, sheet_name='L and LG Interact')\n",
        "\n",
        "# Remove  whitespaces between letters in names of buckling failure modes\n",
        "df['FM'] = df['FM'].str.strip()\n",
        "\n",
        "# label encoding\n",
        "le = LabelEncoder()\n",
        "# Encode the 'Failure modes' name to numbers\n",
        "df['FM'] = le.fit_transform(df['FM'])\n",
        "data_x= df[['L','t','h','b','KL_r','A','Py','Pne','P(crl_s,crd_s)']] # parameters for Partial composite Approach\n",
        "\n",
        "# Create the feature DataFrame\n",
        "X= pd.DataFrame(df,columns=data_x.columns) #input variables\n",
        "\n",
        "# Data Scaling\n",
        "sc_X = StandardScaler()\n",
        "X_scaled = sc_X.fit_transform(X)\n",
        "\n",
        "# Split the data into train and test sets for the classification task\n",
        "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_scaled, df['FM'], test_size=0.3, random_state=123)"
      ],
      "metadata": {
        "id": "Lw0Ubv4T8307"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Multilayer Perceptron (MLP) 0f Artificial Neural Networks classification model\n",
        "\n",
        "# Hyperparameter tuning based on Grid search method\n",
        "# Define the parameter grid\n",
        "#param_grid = {\n",
        "    #'hidden_layer_sizes': [(20, 30, 20, 20, 30), (100, 50), (100,), (50, 100, 50), (150, 100, 50)],\n",
        "    #'activation': ['relu', 'tanh', 'logistic'],\n",
        "    #'solver': ['adam', 'sgd'],\n",
        "    #'alpha': [0.0001,0.001,0.005, 0.01],\n",
        "   #'learning_rate': ['constant', 'adaptive'],\n",
        "    #'learning_rate_init': [0.001, 0.01, 0.1]\n",
        "#}\n",
        "#mlp = MLPClassifier(max_iter=2000, tol=0.0001, random_state=123, early_stopping=True, validation_fraction=0.3, n_iter_no_change=20)\n",
        "\n",
        "# Optimised Hyperparameters (Best parameters selected after tuning)\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50, 100, 50)],\n",
        "    'activation': ['tanh'],\n",
        "    'solver': ['adam'],\n",
        "    'alpha': [0.0001],\n",
        "    'learning_rate': ['constant'],\n",
        "    #'learning_rate_init': [0.1]\n",
        "}\n",
        "\n",
        "mlp = MLPClassifier(max_iter=2000, tol=0.0001, random_state=123)\n",
        "clf = GridSearchCV(estimator=mlp, cv=10, param_grid=param_grid, n_jobs=-1)\n",
        "\n",
        "# Train the classifier on feature and target data\n",
        "clf.fit(X_train_class, y_train_class)\n",
        "\n",
        "# View the best parameters for the model found using grid search\n",
        "print('Best hidden_layer_sizes:',clf.best_estimator_.hidden_layer_sizes)\n",
        "print('Best activation:',clf.best_estimator_.activation)\n",
        "print('Best solver:',clf.best_estimator_.solver)\n",
        "print('Best alpha:',clf.best_estimator_.alpha)\n",
        "print('Best learning_rate:',clf.best_estimator_.learning_rate)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_class_train = clf.predict(X_train_class)\n",
        "y_pred_class_test = clf.predict(X_test_class)"
      ],
      "metadata": {
        "id": "KzqDD_4z868d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################ Performance Metrics ##########################\n",
        "# Calculate accuracy\n",
        "accuracy_train = accuracy_score(y_train_class, y_pred_class_train)\n",
        "accuracy_test = accuracy_score(y_test_class, y_pred_class_test)\n",
        "\n",
        "print('Training accuracy: {:.2f}'.format(accuracy_train))\n",
        "print('Test accuracy: {:.2f}'.format(accuracy_test))\n",
        "\n",
        "# Print confusion matrix\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(y_test_class, y_pred_class_test))\n",
        "\n",
        "# Print classification report\n",
        "print('Classification report:')\n",
        "print(classification_report(y_test_class, y_pred_class_test))\n",
        "\n",
        "#  class names (Inverse Encoding)\n",
        "class_names = le.inverse_transform(np.unique(y_test_class))\n",
        "# Print the class names\n",
        "print('Classes:', class_names)\n",
        "# Get the mapping of class names to numbers\n",
        "class_mapping = {label: index for index, label in enumerate(le.classes_)}\n",
        "# Print the mapping\n",
        "print('Encoding:', class_mapping)"
      ],
      "metadata": {
        "id": "NcGAh7S09-yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classify the failure mode of a new instance after training of MLP ML Model**"
      ],
      "metadata": {
        "id": "30_eM32S-F3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make new prediction\n",
        "# Manually input the new data\n",
        "new_data = {\n",
        "    'L': [1500],         # Length(mm)\n",
        "    't': [1.5],          # Thickness of the section (mm)\n",
        "    'h': [179],          # Height of the section (mm)\n",
        "    'b': [67],           # Flange of the section (mm)\n",
        "    'KL_r': [29.39],     # Non-dimensioanl member slenderness\n",
        "    'Py': [418.5],       # Yielding Strength of Section (kN)\n",
        "    'A': [930],          # Area (mm2) of the section\n",
        "    'Pne': [393.62],     # Global Buckling Strength of the Section (kN)\n",
        "    'P(crl_s,crd_s)': [47.74]  # Sectional (Local or Distortional) Buckling Strength of the section (kN)\n",
        "}\n",
        "\n",
        "# Convert the new input data to a DataFrame\n",
        "new_input_data = pd.DataFrame(new_data)\n",
        "\n",
        "# Ensure the new input data has the same columns as the training data\n",
        "data_x_columns = ['L','t','h','b','KL_r','A','Py','Pne','P(crl_s,crd_s)']\n",
        "new_input_data = new_input_data[data_x_columns]\n",
        "\n",
        "# Scale the new input data using the same scaler used for training data\n",
        "sc_X = StandardScaler()\n",
        "sc_X.fit(X)  # Fit the scaler on the original training data\n",
        "new_input_scaled = sc_X.transform(new_input_data)\n",
        "\n",
        "# Make predictions\n",
        "predicted_class = clf.predict(new_input_scaled)\n",
        "#predicted_probabilities = clf.predict_proba(new_input_scaled)\n",
        "\n",
        "# Decode the predicted class\n",
        "predicted_class_decoded = le.inverse_transform(predicted_class)\n",
        "\n",
        "# Print the results\n",
        "print('Predicted class:', predicted_class_decoded[0])"
      ],
      "metadata": {
        "id": "b16L7Y8h-L01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of the new instance**"
      ],
      "metadata": {
        "id": "AZsZvhvU-V9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP Classification Model Explainer\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "# Create a LIME explainer\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train_class,\n",
        "    feature_names=X.columns,\n",
        "    class_names=class_names,\n",
        "    mode='classification'\n",
        ")\n",
        "\n",
        "# New input data\n",
        "exp = explainer.explain_instance(\n",
        "    data_row=new_input_scaled[0],\n",
        "    predict_fn=clf.predict_proba\n",
        ")\n",
        "\n",
        "# Show the explanation\n",
        "exp.show_in_notebook(show_table=True, show_all=False)\n",
        "\n",
        "# Save the explanation as an HTML file\n",
        "exp.save_to_file('lime_explanation_new_data1.html')"
      ],
      "metadata": {
        "id": "0vOaYg77-R2t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
