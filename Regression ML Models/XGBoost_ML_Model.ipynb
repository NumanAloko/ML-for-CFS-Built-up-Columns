{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfH4Ol35L4FrSUPfoVxxQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NumanAloko/ML-for-CFS-Built-up-Columns/blob/main/Regression%20ML%20Models/XGBoost_ML_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and testing of XGBoost ML model**"
      ],
      "metadata": {
        "id": "pxxwB4IwHp3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that scikit-learn version 1.5 is compatible with the XGBoost regressor; ensure that the version is below 1.6. If a higher version of scikit-learn is installed,  uninstall it.\n",
        "!pip uninstall -y scikit-learn"
      ],
      "metadata": {
        "id": "ajU61MUJ2yG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the compatible version with XGBoost\n",
        "!pip install scikit-learn==1.5.0\n"
      ],
      "metadata": {
        "id": "vvZhK1z44P2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOfNMeGmGx5t"
      },
      "outputs": [],
      "source": [
        "########## Import necessary libraries ########################\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################### Create Dataframe ###############################\n",
        "\n",
        "# Load the dataset\n",
        "csv_url= \"https://raw.githubusercontent.com/NumanAloko/ML-for-CFS-Built-up-Columns/refs/heads/main/CFS_Built-up_Columns_ML_Dataset.csv\"\n",
        "df = pd.read_csv(csv_url, header=0)\n",
        "df = df.dropna(how='all').dropna(axis=1, how='all')\n",
        "\n",
        "data_x = df[['L', 't', 'h', 'b','KL_r','Fy', '位c', '位(le-d)']]  # Input Features\n",
        "y = df[['Pt/Py']]                                               # Target Values\n",
        "\n",
        "# Data Scaling\n",
        "sc_X = StandardScaler()\n",
        "sc_y = StandardScaler()\n",
        "X_sc = sc_X.fit_transform(data_x)\n",
        "y_sc = sc_y.fit_transform(y)\n",
        "\n",
        "# Create DataFrames with the standardised data and keep track of indices\n",
        "X_sc_df = pd.DataFrame(X_sc, columns=data_x.columns, index=data_x.index)\n",
        "y_sc_df = pd.DataFrame(y_sc, columns=['Pt/Py'], index=y.index)\n",
        "\n",
        "# Split the data into training and test sets by the ratio of 70:30.\n",
        "\n",
        "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(\n",
        "    X_sc_df, y_sc_df, X_sc_df.index, test_size=0.3, random_state=123)"
      ],
      "metadata": {
        "id": "iKkdpO6xHBYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the XGBoost model\n",
        "xgb = XGBRegressor(verbosity=0, early_stopping_rounds=10)\n",
        "\n",
        "# Hyperparameter tuning based on Grid search method\n",
        "# Define the parameter grid\n",
        "#param_grid = {\n",
        "    #'n_estimators': [100, 200, 300, 500],\n",
        "    #'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    #'max_depth': [3, 6, 9, 12],\n",
        "    #'subsample': [0.5, 0.7, 0.9, 1.0],\n",
        "    #'colsample_bytree': [0.5, 0.7, 0.9, 1.0],\n",
        "    #'reg_alpha': [0, 0.1, 0.5, 1],  # L1 regularization\n",
        "    #'reg_lambda': [1, 1.5, 2, 3],   # L2 regularization\n",
        "    #'objective': ['reg:squarederror']\n",
        "#}\n",
        "\n",
        "# Optimised Hyperparameters (Best parameters selected after tuning)\n",
        "param_grid = {\n",
        "    'n_estimators': [500],\n",
        "    'learning_rate': [0.2],\n",
        "    'max_depth': [3],\n",
        "    'subsample': [1.0],\n",
        "    'colsample_bytree': [0.7],\n",
        "    'reg_alpha': [0.5],  # L1 regularization\n",
        "    'reg_lambda': [3],   # L2 regularization\n",
        "    'objective': ['reg:squarederror']\n",
        "}\n",
        "\n",
        "#Initialise GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "\n",
        "# Fit the grid search\n",
        "grid_search.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters found: \", best_params)"
      ],
      "metadata": {
        "id": "DRCs8z0wrdhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Make predictions on the training and test sets\n",
        "y_pred_xgb_train = grid_search.predict(X_train)\n",
        "y_pred_xgb_test = grid_search.predict(X_test)\n",
        "\n",
        "# De-normalise predictions\n",
        "y_pred_xgb_train_de = pd.DataFrame(y_pred_xgb_train * sc_y.scale_ + sc_y.mean_, columns=y_train.columns, index=y_train.index)\n",
        "y_pred_xgb_test_de = pd.DataFrame(y_pred_xgb_test * sc_y.scale_ + sc_y.mean_, columns=y_test.columns, index=y_test.index)\n",
        "\n",
        "# Combine again all true target values and predictions of XGBoost model\n",
        "y_all_de = pd.concat([y.loc[train_indices], y.loc[test_indices]])\n",
        "y_pred_all_de = pd.concat([y_pred_xgb_train_de, y_pred_xgb_test_de])"
      ],
      "metadata": {
        "id": "Yet2dEfLHP5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################  Performance Indicators ########################################\n",
        "\n",
        "# Coefficient of determination (r2)\n",
        "r2_train = r2_score(y.loc[train_indices], y_pred_xgb_train_de)\n",
        "r2_test = r2_score(y.loc[test_indices], y_pred_xgb_test_de)\n",
        "r2_all = r2_score(y_all_de, y_pred_all_de)\n",
        "\n",
        "# Calculation of RMSE (Root mean squared error) for training data\n",
        "rmse_train = mean_squared_error(y.loc[train_indices], y_pred_xgb_train_de, squared=False)\n",
        "# Calculation of RMSE  (Root mean squared error) for test data\n",
        "rmse_test = mean_squared_error(y.loc[test_indices], y_pred_xgb_test_de, squared=False)\n",
        "# Calculation of RMSE  (Root mean squared error)for all data\n",
        "rmse_all = mean_squared_error(y_all_de, y_pred_all_de, squared=False)\n",
        "\n",
        "print(f'r2 score for training = {r2_train:.4f}')\n",
        "print(f'r2 score for testing = {r2_test:.4f}')\n",
        "print(f'r2 score for all = {r2_all:.4f}')\n",
        "\n",
        "print(f'RMSE score for training = {rmse_train:.4f}')\n",
        "print(f'RMSE score for testing = {rmse_test:.4f}')\n",
        "print(f'RMSE score for all = {rmse_all:.4f}')\n",
        "\n",
        "# # Calculation of ratios of true target values to XGBoost ML model's predicted values\n",
        "ratio_train_de = y.loc[train_indices] / y_pred_xgb_train_de\n",
        "ratio_test_de = y.loc[test_indices] / y_pred_xgb_test_de\n",
        "ratio_all_de = y_all_de / y_pred_all_de\n",
        "\n",
        "# Mean of ratios\n",
        "mean_ratio_train_de = ratio_train_de.mean(axis=0)\n",
        "mean_ratio_test_de = ratio_test_de.mean(axis=0)\n",
        "mean_ratio_all_de = ratio_all_de.mean(axis=0)\n",
        "\n",
        "# Standard deviation of ratios\n",
        "std_ratio_train_de = ratio_train_de.std(axis=0)\n",
        "std_ratio_test_de = ratio_test_de.std(axis=0)\n",
        "std_ratio_all_de = ratio_all_de.std(axis=0)\n",
        "\n",
        "print(f'Mean of ratio for training (de-normalized) = {mean_ratio_train_de.mean():.4f}')\n",
        "print(f'Mean of ratio for testing (de-normalized) = {mean_ratio_test_de.mean():.4f}')\n",
        "print(f'Mean of ratio for all data (de-normalized) = {mean_ratio_all_de.mean():.4f}')\n",
        "\n",
        "print(f'Standard deviation of ratio for training (de-normalized) = {std_ratio_train_de.mean():.4f}')\n",
        "print(f'Standard deviation of ratio for testing (de-normalized) = {std_ratio_test_de.mean():.4f}')\n",
        "print(f'Standard deviation of ratio for all data (de-normalized) = {std_ratio_all_de.mean():.4f}')"
      ],
      "metadata": {
        "id": "KNCy_oPLHYyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make new prediction after training of XGBoost ML Model**"
      ],
      "metadata": {
        "id": "R-zblpS1Hyke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make new prediction\n",
        "# Manually input the new data\n",
        "new_data = {\n",
        "    'L': [1500],               # Length(mm)\n",
        "    't': [1.5],                # Thickness of the section (mm)\n",
        "    'h': [175],                # Height of the single section (mm)\n",
        "    'b': [65],                 # Flange of the single section (mm)\n",
        "    'KL_r': [25.27],           # Non-dimensioanl member slenderness\n",
        "    'Fy': [450],               # Yield stress\n",
        "    '位c': [0.38],              # Global Slenderness\n",
        "    '位(le-d)': [2.36],         # Sectional Slenderness\n",
        "    'A': [988]                 # Area\n",
        "}\n",
        "\n",
        "# Create a DataFrame for the new data\n",
        "new_df = pd.DataFrame(new_data)\n",
        "\n",
        "area = new_df['A'].values[0]\n",
        "fy = new_df['Fy'].values[0]\n",
        "\n",
        "new_df_features = new_df.drop(columns=['A'])\n",
        "\n",
        "# Scale the new data using the same scaler used for training data\n",
        "new_data_x_scaled = sc_X.transform(new_df_features)\n",
        "\n",
        "# Make predictions on the new data\n",
        "new_predictions_scaled = grid_search.predict(new_data_x_scaled)\n",
        "\n",
        "# Reverse the scaling on the predictions\n",
        "new_predictions = new_predictions_scaled * sc_y.scale_ + sc_y.mean_\n",
        "\n",
        "# Multiply the prediction (Pt/Py) by yielding strength(area x yield stress) of Section area and yield strength\n",
        "new_predictions = new_predictions * (area * fy *0.001)\n",
        "\n",
        "# Create a DataFrame for the final predictions\n",
        "new_predictions_df = pd.DataFrame(new_predictions, columns=['Predicted'])\n",
        "\n",
        "# Print the final predictions\n",
        "print(\"Predicted Axial Strength:\", new_predictions_df['Predicted'].values)"
      ],
      "metadata": {
        "id": "PUnE3aNSHbmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretion of XGBoost ML model's prediction for new instance**"
      ],
      "metadata": {
        "id": "Bt3MA--YH5a-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpretation of New Instance\n",
        "import shap\n",
        "\n",
        "# Initialise the explainer and compute SHAP values for the new input\n",
        "explainer = shap.KernelExplainer(grid_search.predict, shap.sample(X_train, 725))\n",
        "new_shap_values = explainer.shap_values(new_data_x_scaled)\n",
        "\n",
        "# Convert the new input data to a DataFrame with the new feature names\n",
        "new_feature_names = ['L', 't', 'h', 'b', 'KL/r', '$F_y$', '$\\\\lambda_c$', '$\\\\lambda_{(le,d)}$']\n",
        "new_input_df = pd.DataFrame(new_data_x_scaled, columns=new_feature_names)\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# The decision plot for the new instance\n",
        "shap.decision_plot(explainer.expected_value, new_shap_values[0], new_input_df.iloc[0], feature_names=new_feature_names, highlight=0, show=False)\n",
        "\n",
        "# Get the current figure and axes\n",
        "fig = plt.gcf()\n",
        "ax = plt.gca()\n",
        "\n",
        "# Set the x limits and the y label\n",
        "#ax.set_xlim(-1.8, 1.0)\n",
        "ax.set_ylabel('Input Features', fontsize=14)\n",
        "ax.set_title('Interpretation of new instance', fontsize=14)\n",
        "\n",
        "# Add small ticks on the y-axis\n",
        "ax.tick_params(axis='y', direction='out', length=6, width=2, colors='black')\n",
        "\n",
        "# Save the figure\n",
        "ax.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1eRU3BecHgDK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}